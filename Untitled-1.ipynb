{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_index import SimpleDirectoryReader, GPTListIndex, GPTSimpleVectorIndex, LLMPredictor, PromptHelper\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import gradio as gr\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_index(directory_path):\n",
    "    max_input_size = 4096\n",
    "    num_outputs = 512\n",
    "    max_chunk_overlap = 20\n",
    "    chunk_size_limit = 600\n",
    "\n",
    "    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)\n",
    "\n",
    "    llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.7, model_name=\"gpt-3.5-turbo\", max_tokens=num_outputs))\n",
    "\n",
    "    documents = SimpleDirectoryReader(directory_path).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1331847561.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    index.save_to_disk('index.json')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "index = GPTSimpleVectorIndex(documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
    "\n",
    "    index.save_to_disk('index.json')\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m      6\u001b[0m iface \u001b[39m=\u001b[39m gr\u001b[39m.\u001b[39mInterface(fn\u001b[39m=\u001b[39mchatbot,\n\u001b[1;32m      7\u001b[0m                      inputs\u001b[39m=\u001b[39mgr\u001b[39m.\u001b[39mcomponents\u001b[39m.\u001b[39mTextbox(lines\u001b[39m=\u001b[39m\u001b[39m7\u001b[39m, label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEnter your text\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m      8\u001b[0m                      outputs\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m                      title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCustom-trained AI Chatbot\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m index \u001b[39m=\u001b[39m construct_index(\u001b[39m\"\u001b[39m\u001b[39mdocs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m iface\u001b[39m.\u001b[39;49mlaunch(share\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/Documents/thinkmetrics-ai/.venv/lib/python3.9/site-packages/gradio/blocks.py:1932\u001b[0m, in \u001b[0;36mBlocks.launch\u001b[0;34m(self, inline, inbrowser, share, debug, enable_queue, max_threads, auth, auth_message, prevent_thread_lock, show_error, server_name, server_port, show_tips, height, width, encrypt, favicon_path, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_verify, quiet, show_api, file_directories, allowed_paths, blocked_paths, root_path, _frontend, app_kwargs)\u001b[0m\n\u001b[1;32m   1930\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1931\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshare_url \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1932\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshare_url \u001b[39m=\u001b[39m networking\u001b[39m.\u001b[39;49msetup_tunnel(\n\u001b[1;32m   1933\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mserver_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mserver_port, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshare_token\n\u001b[1;32m   1934\u001b[0m         )\n\u001b[1;32m   1935\u001b[0m     \u001b[39mprint\u001b[39m(strings\u001b[39m.\u001b[39men[\u001b[39m\"\u001b[39m\u001b[39mSHARE_LINK_DISPLAY\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshare_url))\n\u001b[1;32m   1936\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (quiet):\n",
      "File \u001b[0;32m~/Documents/thinkmetrics-ai/.venv/lib/python3.9/site-packages/gradio/networking.py:190\u001b[0m, in \u001b[0;36msetup_tunnel\u001b[0;34m(local_host, local_port, share_token)\u001b[0m\n\u001b[1;32m    186\u001b[0m     remote_host, remote_port \u001b[39m=\u001b[39m payload[\u001b[39m\"\u001b[39m\u001b[39mhost\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39mint\u001b[39m(payload[\u001b[39m\"\u001b[39m\u001b[39mport\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    187\u001b[0m     tunnel \u001b[39m=\u001b[39m Tunnel(\n\u001b[1;32m    188\u001b[0m         remote_host, remote_port, local_host, local_port, share_token\n\u001b[1;32m    189\u001b[0m     )\n\u001b[0;32m--> 190\u001b[0m     address \u001b[39m=\u001b[39m tunnel\u001b[39m.\u001b[39;49mstart_tunnel()\n\u001b[1;32m    191\u001b[0m     \u001b[39mreturn\u001b[39;00m address\n\u001b[1;32m    192\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/thinkmetrics-ai/.venv/lib/python3.9/site-packages/gradio/tunneling.py:59\u001b[0m, in \u001b[0;36mTunnel.start_tunnel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstart_tunnel\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m     58\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownload_binary()\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_start_tunnel(BINARY_PATH)\n\u001b[1;32m     60\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl\n",
      "File \u001b[0;32m~/Documents/thinkmetrics-ai/.venv/lib/python3.9/site-packages/gradio/tunneling.py:95\u001b[0m, in \u001b[0;36mTunnel._start_tunnel\u001b[0;34m(self, binary)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproc\u001b[39m.\u001b[39mstdout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     94\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m line \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproc\u001b[39m.\u001b[39;49mstdout\u001b[39m.\u001b[39;49mreadline()\n\u001b[1;32m     96\u001b[0m line \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     97\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mstart proxy success\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m line:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def chatbot(input_text):\n",
    "    index = GPTSimpleVectorIndex.load_from_disk('index.json')\n",
    "    response = index.query(input_text, response_mode=\"compact\")\n",
    "    return response.response\n",
    "\n",
    "iface = gr.Interface(fn=chatbot,\n",
    "                     inputs=gr.components.Textbox(lines=7, label=\"Enter your text\"),\n",
    "                     outputs=\"text\",\n",
    "                     title=\"Custom-trained AI Chatbot\")\n",
    "\n",
    "index = construct_index(\"docs\")\n",
    "iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_INSTRUCTOR_XL = \"hkunlp/instructor-xl\"\n",
    "EMB_SBERT_MPNET_BASE = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "LLM_FLAN_T5_XXL = \"google/flan-t5-xxl\"\n",
    "LLM_FLAN_T5_XL = \"google/flan-t5-xl\"\n",
    "LLM_FASTCHAT_T5_XL = \"lmsys/fastchat-t5-3b-v1.0\"\n",
    "LLM_FLAN_T5_SMALL = \"google/flan-t5-small\"\n",
    "LLM_FLAN_T5_BASE = \"google/flan-t5-base\"\n",
    "LLM_FLAN_T5_LARGE = \"google/flan-t5-large\"\n",
    "LLM_FALCON_SMALL = \"tiiuae/falcon-7b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"persist_directory\":None,\n",
    "          \"load_in_8bit\":False,\n",
    "          \"embedding\" : EMB_SBERT_MPNET_BASE,\n",
    "          \"llm\":LLM_FLAN_T5_BASE,\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sbert_mpnet():\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        return HuggingFaceEmbeddings(model_name=EMB_SBERT_MPNET_BASE, model_kwargs={\"device\": device})\n",
    "\n",
    "\n",
    "def create_flan_t5_base(load_in_8bit=False):\n",
    "        # Wrap it in HF pipeline for use with LangChain\n",
    "        model=\"google/flan-t5-base\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        return pipeline(\n",
    "            task=\"text2text-generation\",\n",
    "            model=model,\n",
    "            tokenizer = tokenizer,\n",
    "            max_new_tokens=100,\n",
    "            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "if config[\"embedding\"] == EMB_SBERT_MPNET_BASE:\n",
    "    embedding = create_sbert_mpnet()\n",
    "load_in_8bit = config[\"load_in_8bit\"]\n",
    "if config[\"llm\"] == LLM_FLAN_T5_BASE:\n",
    "    llm = create_flan_t5_base(load_in_8bit=load_in_8bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_falcon_instruct_small(load_in_8bit=False):\n",
    "        model = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        hf_pipeline = pipeline(\n",
    "                task=\"text-generation\",\n",
    "                model = model,\n",
    "                tokenizer = tokenizer,\n",
    "                trust_remote_code = True,\n",
    "                max_new_tokens=100,\n",
    "                model_kwargs={\n",
    "                    \"device_map\": \"auto\", \n",
    "                    \"load_in_8bit\": load_in_8bit, \n",
    "                    \"max_length\": 512, \n",
    "                    \"temperature\": 0.01,\n",
    "                    \"torch_dtype\":torch.bfloat16,\n",
    "                    }\n",
    "            )\n",
    "        return hf_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pdf\n",
    "pdf_path = \"wiki_data_short.pdf\"\n",
    "loader = PDFPlumberLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "\n",
    "# Split documents and create text snippets\n",
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=10, encoding_name=\"cl100k_base\")  # This the encoding for text-embedding-ada-002\n",
    "texts = text_splitter.split_documents(texts)\n",
    "\n",
    "persist_directory = config[\"persist_directory\"]\n",
    "vectordb = Chroma.from_documents(documents=texts, embedding=embedding, persist_directory=persist_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_llm = HuggingFacePipeline(pipeline=llm)\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\":4})\n",
    "qa = RetrievalQA.from_chain_type(llm=hf_llm, chain_type=\"stuff\",retriever=retriever)\n",
    "\n",
    "# Defining a default prompt for flan models\n",
    "if config[\"llm\"] == LLM_FLAN_T5_SMALL or config[\"llm\"] == LLM_FLAN_T5_BASE or config[\"llm\"] == LLM_FLAN_T5_LARGE:\n",
    "    question_t5_template = \"\"\"\n",
    "    context: {context}\n",
    "    question: {question}\n",
    "    answer: \n",
    "    \"\"\"\n",
    "    QUESTION_T5_PROMPT = PromptTemplate(\n",
    "        template=question_t5_template, input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    qa.combine_documents_chain.llm_chain.prompt = QUESTION_T5_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what's the reason for financial crisis?\"\n",
    "qa.combine_documents_chain.verbose = True\n",
    "qa.return_source_documents = True\n",
    "qa({\"query\":question,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PdfQA:\n",
    "    def __init__(self,config:dict = {}):\n",
    "        self.config = config\n",
    "        self.embedding = None\n",
    "        self.vectordb = None\n",
    "        self.llm = None\n",
    "        self.qa = None\n",
    "        self.retriever = None\n",
    "\n",
    "    ...\n",
    "# Check out the full script on the Github link on the intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for PdfQA\n",
    "config = {\"persist_directory\":None,\n",
    "          \"load_in_8bit\":False,\n",
    "          \"embedding\" : EMB_SBERT_MPNET_BASE,\n",
    "          \"llm\":LLM_FLAN_T5_BASE,\n",
    "          \"pdf_path\":\"wiki_data_short.pdf\"\n",
    "          }\n",
    "\n",
    "# Initialize PdfQA\n",
    "pdfqa = PdfQA(config=config)\n",
    "pdfqa.init_embeddings()\n",
    "pdfqa.init_models()\n",
    "\n",
    "# Create Vector DB \n",
    "pdfqa.vector_db_pdf()\n",
    "\n",
    "# Set up Retrieval QA Chain\n",
    "pdfqa.retreival_qa_chain()\n",
    "\n",
    "# Query the model\n",
    "question = \"what the reason for financial crisis?\"\n",
    "pdfqa.answer_query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from pdf_qa import PdfQA\n",
    "from pathlib import Path\n",
    "from tempfile import NamedTemporaryFile\n",
    "import time\n",
    "import shutil\n",
    "from constants import * ## constants.py file can be found in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamlit app code\n",
    "st.set_page_config(\n",
    "    page_title='Q&A Bot for PDF',\n",
    "    page_icon='ðŸ”–',\n",
    "    layout='wide',\n",
    "    initial_sidebar_state='auto',\n",
    ")\n",
    "\n",
    "\n",
    "if \"pdf_qa_model\" not in st.session_state:\n",
    "    st.session_state[\"pdf_qa_model\"]:PdfQA = PdfQA() ## Intialisation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
